dilated_ratio:  [1, 2, 4, 8, 16]
segment_length:  [1024, 5792, 32768, 185363, 1048576]
Number of trainable LongNet parameters:  85148160
Global Pooling: False
slide_encoder.pth: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 345M/345M [00:35<00:00, 9.68MB/s]
[92m Successfully Loaded Pretrained GigaPath model from hf_hub:prov-gigapath/prov-gigapath [00m
[768, 512, 256]
  0%|                                                                                                       | 0/518 [00:00<?, ?it/s]
2
<built-in method size of Tensor object at 0x742f5f920d10>
1 1 768
tensor([[[-0.1111, -0.0097, -0.0495,  0.0722, -0.1013,  0.0435, -0.0551,
           0.0571, -0.0814,  0.0389, -0.0173, -0.0304,  0.0174,  0.0779,
           0.0343, -0.0091, -0.0127,  0.0570,  0.0940,  0.0580, -0.0015,
           0.0540, -0.0400,  0.0997, -0.0612,  0.0090,  0.0613,  0.0007,
          -0.0436,  0.0636,  0.1142,  0.1320, -0.0026,  0.0034,  0.0398,
          -0.0842, -0.0631,  0.0180, -0.0828, -0.0180,  0.0522,  0.1123,
          -0.0341,  0.0057,  0.0068, -0.0383, -0.0695,  0.1068,  0.1062,
           0.0862,  0.0372, -0.0281, -0.0426, -0.0429,  0.0654, -0.0720,
           0.0577,  0.0565, -0.0167,  0.0047,  0.0234, -0.0239, -0.0209,
           0.0605,  0.0710,  0.0398, -0.0022, -0.0562, -0.0914,  0.0449,
          -0.0505,  0.0484,  0.0464, -0.0025,  0.0191,  0.0565,  0.0028,
          -0.0083,  0.0561,  0.0515, -0.0380,  0.1105, -0.0318, -0.0975,
          -0.1020, -0.0715,  0.1081, -0.0142,  0.0408,  0.0115,  0.0354,
           0.0222, -0.0047,  0.0667, -0.0628, -0.0991, -0.0027, -0.0628,
           0.0012,  0.0188, -0.0885, -0.0496,  0.0127, -0.0055, -0.1773,
           0.0522, -0.0571,  0.0793,  0.0914,  0.0046,  0.0433,  0.0699,
          -0.0147,  0.0291,  0.0294, -0.0330,  0.1142, -0.0440, -0.0315,
          -0.1022, -0.0742, -0.0789,  0.0978, -0.0011, -0.0261, -0.0434,
          -0.0474,  0.0475,  0.0056,  0.0319, -0.0521, -0.0580, -0.0104,
          -0.1583,  0.1452, -0.0933,  0.0109,  0.1130, -0.0346, -0.0288,
          -0.0712, -0.0891, -0.0500,  0.0004, -0.0114, -0.0906, -0.0461,
           0.0848,  0.0354,  0.0547,  0.0151, -0.1766,  0.0197,  0.0366,
           0.0981, -0.0284,  0.0626,  0.0705, -0.0062,  0.0880,  0.0771,
           0.0571, -0.1249, -0.0269, -0.0183, -0.0057,  0.0433, -0.0459,
           0.0658,  0.0292,  0.1173, -0.1173, -0.0244,  0.0167,  0.1244,
           0.1791,  0.0149,  0.0491, -0.0560, -0.0504, -0.0417, -0.0299,
           0.0219, -0.0022, -0.0347, -0.0349, -0.0913,  0.0401, -0.0914,
          -0.0669, -0.0534,  0.0171,  0.0033, -0.0469, -0.0416,  0.1009,
           0.0611,  0.0714, -0.0672, -0.0979,  0.0527,  0.0950,  0.0431,
           0.0747,  0.0785,  0.1298,  0.0251, -0.0312, -0.0537,  0.0195,
          -0.0310, -0.0349,  0.0216,  0.0179,  0.0336,  0.0394,  0.0323,
          -0.0526, -0.0497, -0.0359,  0.0821,  0.0839,  0.0105, -0.0287,
          -0.0049,  0.0595, -0.0035, -0.0443,  0.0597,  0.0662,  0.0210,
           0.0021, -0.0761,  0.0075,  0.0331,  0.1570, -0.0292,  0.1015,
          -0.0364,  0.0003, -0.0407,  0.0044, -0.0090,  0.0626,  0.0042,
           0.0745,  0.0649,  0.0277,  0.0217, -0.1093, -0.0114,  0.0669,
          -0.0693,  0.0603, -0.0680, -0.0683,  0.0747, -0.0944, -0.0584,
           0.0725,  0.0199,  0.0282,  0.0847, -0.0115, -0.0050,  0.0398,
           0.0174, -0.0790,  0.0332, -0.0378,  0.0913,  0.0867,  0.1870,
           0.0462, -0.0393, -0.0201, -0.1156,  0.0396, -0.0817,  0.0075,
          -0.0144, -0.0046, -0.0982, -0.0877, -0.0307,  0.0346, -0.0254,
          -0.0934,  0.0200,  0.1745,  0.2050,  0.0660,  0.0515,  0.0334,
           0.0071,  0.0352,  0.0391,  0.0532,  0.0283,  0.0904, -0.0380,
           0.0647, -0.1856, -0.0184, -0.0008,  0.1039, -0.0326, -0.0352,
           0.0241, -0.0618, -0.0292,  0.0996, -0.0554, -0.0666,  0.0427,
          -0.0434, -0.0319,  0.1292,  0.1016,  0.0381,  0.0434,  0.0870,
          -0.0505, -0.0221, -0.0974,  0.0429,  0.0281,  0.1532, -0.0575,
          -0.0542,  0.0633,  0.1425, -0.0603, -0.0800, -0.0328, -0.0181,
           0.0605,  0.0391,  0.0185, -0.0107, -0.0217, -0.0840, -0.0042,
           0.0122,  0.0115, -0.0728,  0.0096,  0.0535, -0.1584, -0.0371,
           0.0136, -0.0284, -0.1094,  0.1800,  0.0794, -0.0475, -0.0101,
           0.0610,  0.0952,  0.0545,  0.1004, -0.0306,  0.0784,  0.0782,
           0.0332,  0.0845, -0.0964, -0.0711, -0.0347,  0.0332, -0.0044,
          -0.0122,  0.0103, -0.1289,  0.0596,  0.0770,  0.0061,  0.0039,
           0.0425,  0.0473, -0.0639,  0.0457, -0.0455,  0.0532, -0.0241,
           0.0427,  0.0243,  0.0035, -0.0387,  0.1407,  0.1203,  0.1083,
           0.0113, -0.0214, -0.1325, -0.0317,  0.0051,  0.0119, -0.0159,
           0.0746,  0.0141, -0.0237,  0.0009,  0.0170, -0.0090, -0.0138,
           0.0143,  0.0484, -0.0040, -0.0083, -0.0020, -0.1311, -0.0088,
           0.0466,  0.0664, -0.0859, -0.0077,  0.0005,  0.0210, -0.0703,
          -0.0568,  0.0015, -0.0717,  0.0077, -0.0359,  0.1027, -0.1024,
          -0.0941,  0.0310, -0.0327,  0.0204, -0.0775, -0.0265, -0.0158,
           0.0545, -0.0599,  0.0777,  0.0499,  0.0067, -0.0760, -0.1200,
           0.0677,  0.0536, -0.1465,  0.0334, -0.0802,  0.0574, -0.0283,
           0.0618, -0.0140, -0.0511,  0.0335,  0.0882,  0.0588, -0.0063,
          -0.0479, -0.0455, -0.1145, -0.0104,  0.0504, -0.0374, -0.0747,
           0.0103,  0.0548, -0.0089,  0.0098, -0.0015,  0.0385,  0.0270,
          -0.1362, -0.0644,  0.0116,  0.0058, -0.0641, -0.0627, -0.0234,
           0.0483, -0.1278, -0.0880,  0.0050,  0.0024, -0.0088, -0.0674,
           0.0864, -0.0342,  0.0647,  0.1106,  0.0953, -0.0755, -0.0311,
           0.0111, -0.0760,  0.0458, -0.0350, -0.0478,  0.0694, -0.0936,
          -0.0225, -0.0657,  0.0921,  0.0030,  0.0066,  0.1372, -0.0659,
          -0.0131,  0.0809,  0.0717,  0.0777,  0.0034,  0.0818, -0.0233,
           0.0302, -0.0197, -0.0118,  0.0925, -0.0070,  0.0026,  0.0867,
          -0.0534, -0.0225, -0.0319,  0.0528,  0.0151, -0.0702, -0.0767,
          -0.1491,  0.0477,  0.0853,  0.0768,  0.1287, -0.1397,  0.1014,
          -0.0144, -0.0451,  0.0943, -0.0158, -0.0475, -0.0392, -0.0422,
           0.1194, -0.0201, -0.0404,  0.1395, -0.1271, -0.0801, -0.1025,
           0.0007, -0.0508, -0.0491, -0.0874, -0.0965,  0.0743, -0.0231,
          -0.0237, -0.0604, -0.0326, -0.0537, -0.0595, -0.1520,  0.0210,
          -0.0258,  0.1334, -0.0616, -0.0047, -0.0599, -0.0644,  0.0336,
           0.0162,  0.0899,  0.0245,  0.0012, -0.0430, -0.0451,  0.0450,
          -0.0710,  0.0736,  0.1469,  0.0813,  0.0158, -0.0279,  0.0271,
          -0.0229, -0.0176, -0.1202,  0.0350, -0.0554,  0.0318,  0.0716,
           0.0817, -0.1022,  0.0767, -0.0424, -0.0080,  0.0686, -0.1006,
           0.0365,  0.0363,  0.0081,  0.0096, -0.0176, -0.0742, -0.1156,
           0.0338, -0.0129, -0.0799,  0.0019,  0.1517, -0.0477,  0.0772,
           0.0087,  0.0689,  0.0930,  0.0657, -0.0072,  0.1339, -0.0548,
           0.0335,  0.0768, -0.1216, -0.1139,  0.0207,  0.1030, -0.0291,
          -0.0822,  0.0061,  0.0851, -0.0389, -0.0068, -0.1101,  0.0297,
          -0.0536, -0.0685, -0.0041,  0.0861, -0.0280, -0.0344,  0.0063,
          -0.0699,  0.1280, -0.0449, -0.0857, -0.0863,  0.0221,  0.0617,
           0.0644,  0.0145,  0.0422,  0.0162,  0.0269, -0.1255,  0.0354,
          -0.0100, -0.0172,  0.0326, -0.0237,  0.1283,  0.0133,  0.0716,
          -0.1482, -0.0040, -0.0399, -0.0367, -0.0249,  0.0531,  0.0091,
           0.0394, -0.0842, -0.0440,  0.0332, -0.0287, -0.1350, -0.0278,
          -0.0414,  0.0218, -0.0918,  0.0293,  0.0122,  0.0389,  0.1113,
           0.0371, -0.0963, -0.0027, -0.0576,  0.0959,  0.0555,  0.0394,
           0.0404, -0.0161, -0.0520, -0.1001,  0.0023, -0.0733, -0.0289,
           0.0259, -0.0138, -0.0120,  0.0520,  0.0524, -0.0168,  0.0560,
           0.0354, -0.0152,  0.0758,  0.0142, -0.0165, -0.0279,  0.0935,
          -0.0017, -0.0124, -0.0552, -0.1376,  0.0039,  0.0683, -0.0201,
          -0.0892,  0.0223,  0.0789, -0.0853, -0.0051,  0.0219, -0.0067,
           0.0360, -0.0334, -0.0878, -0.0238,  0.1042, -0.0163, -0.0215,
           0.0255,  0.1132, -0.0613,  0.0227, -0.0296,  0.0470, -0.1302,
           0.0595, -0.0158, -0.0406,  0.0679, -0.0637, -0.0650, -0.0263,
           0.0036,  0.1562,  0.1286,  0.0857,  0.0057, -0.0296, -0.0315,
          -0.0229,  0.1612,  0.0241,  0.0544, -0.0230, -0.0108,  0.1096,
           0.1485, -0.0531,  0.0552,  0.1539, -0.0640, -0.1063,  0.0219,
           0.0744, -0.1701,  0.0195, -0.0857, -0.1433]]], device='cuda:0',
       grad_fn=<CudnnRnnBackward0>)
<built-in method size of Tensor object at 0x742f5f920d10>
1 1 768
tensor([[[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
          nan, nan, nan, nan, nan, nan, nan, nan, nan]]], device='cuda:0',
       grad_fn=<CudnnRnnBackward0>)
Traceback (most recent call last):
  File "/work/PAMIL_two_round/train.py", line 88, in <module>
    main(args)
  File "/work/PAMIL_two_round/train.py", line 81, in main
    train(args,basedmodel,ppo,classifier_chief, classifier_giga,FusionHisF, gigapath_model, memory,train_dataloader, validation_dataloader, test_dataloader, wandb)
  File "/work/PAMIL_two_round/utilmodule/core.py", line 358, in train
    ppo.update(memory)
  File "/work/PAMIL_GIGAPATH_CHIEF/models/DPSF.py", line 243, in update
    logprobs, state_values, dist_entropy = self.policy.evaluate(old_msg_states, old_actions)
  File "/work/PAMIL_GIGAPATH_CHIEF/models/DPSF.py", line 177, in evaluate
    dist = torch.distributions.multivariate_normal.MultivariateNormal(action_mean, scale_tril=cov_mat)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributions/multivariate_normal.py", line 150, in __init__
    super().__init__(batch_shape, event_shape, validate_args=validate_args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributions/distribution.py", line 62, in __init__
    raise ValueError(
ValueError: Expected parameter loc (Tensor of shape (1, 30)) of distribution MultivariateNormal(loc: torch.Size([1, 30]), scale_tril: torch.Size([1, 30, 30])) to satisfy the constraint IndependentConstraint(Real(), 1), but found invalid values:
tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
         nan, nan, nan, nan, nan, nan]], device='cuda:0',
       grad_fn=<ExpandBackward0>)
